{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1782442,"sourceType":"datasetVersion","datasetId":1059701}],"dockerImageVersionId":30513,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl","metadata":{"id":"RX4ejp_Y1RiW","outputId":"f73a12a8-d6bd-412c-c4f4-97f0bfd1bb53","execution":{"iopub.status.busy":"2023-07-25T18:09:42.042205Z","iopub.execute_input":"2023-07-25T18:09:42.043107Z","iopub.status.idle":"2023-07-25T18:10:23.538539Z","shell.execute_reply.started":"2023-07-25T18:09:42.043063Z","shell.execute_reply":"2023-07-25T18:10:23.537563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing TPU","metadata":{}},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"id":"C_VrY6O41RiZ","outputId":"4e4afe5e-1ed8-4763-f0fb-ac7e926b560c","execution":{"iopub.status.busy":"2023-07-25T18:10:23.540198Z","iopub.execute_input":"2023-07-25T18:10:23.540772Z","iopub.status.idle":"2023-07-25T18:10:32.515305Z","shell.execute_reply.started":"2023-07-25T18:10:23.540742Z","shell.execute_reply":"2023-07-25T18:10:32.514296Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar100\n\n(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='coarse')","metadata":{"id":"L3yzyaxO1RiZ","outputId":"ebd4f5fb-51c8-4f43-ba0a-1d5c1488723b","execution":{"iopub.status.busy":"2023-07-25T18:10:32.516472Z","iopub.execute_input":"2023-07-25T18:10:32.516766Z","iopub.status.idle":"2023-07-25T18:10:37.239109Z","shell.execute_reply.started":"2023-07-25T18:10:32.51674Z","shell.execute_reply":"2023-07-25T18:10:37.23762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","metadata":{"id":"69ey-ck01Ria","outputId":"d348c7e6-51bc-4e22-cc24-d59e4119564a","execution":{"iopub.status.busy":"2023-07-25T18:10:37.241897Z","iopub.execute_input":"2023-07-25T18:10:37.242221Z","iopub.status.idle":"2023-07-25T18:10:37.247904Z","shell.execute_reply.started":"2023-07-25T18:10:37.242195Z","shell.execute_reply":"2023-07-25T18:10:37.246887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resizing images","metadata":{}},{"cell_type":"code","source":"import cv2\nX_train = np.array([cv2.resize(img, (224, 224)) for img in X_train])","metadata":{"id":"2kAH50ZX1Rib","execution":{"iopub.status.busy":"2023-07-25T18:10:37.248941Z","iopub.execute_input":"2023-07-25T18:10:37.249204Z","iopub.status.idle":"2023-07-25T18:10:49.899609Z","shell.execute_reply.started":"2023-07-25T18:10:37.249181Z","shell.execute_reply":"2023-07-25T18:10:49.898171Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = np.array([cv2.resize(img, (224, 224)) for img in X_test])","metadata":{"id":"Wy328zba1Ric","execution":{"iopub.status.busy":"2023-07-25T18:10:49.9011Z","iopub.execute_input":"2023-07-25T18:10:49.901438Z","iopub.status.idle":"2023-07-25T18:10:51.060147Z","shell.execute_reply.started":"2023-07-25T18:10:49.901409Z","shell.execute_reply":"2023-07-25T18:10:51.058719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoding labels","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder()\ny_train=enc.fit_transform(y_train).toarray().astype(int)\ny_test=enc.transform(y_test).toarray().astype(int)\n\n\nprint(y_train.shape)\nprint(y_train[0])","metadata":{"id":"gPS8d_A51Rid","outputId":"4a7bc3e3-983c-4753-b72c-63b03e226214","execution":{"iopub.status.busy":"2023-07-25T18:10:51.06159Z","iopub.execute_input":"2023-07-25T18:10:51.061901Z","iopub.status.idle":"2023-07-25T18:10:51.439536Z","shell.execute_reply.started":"2023-07-25T18:10:51.061874Z","shell.execute_reply":"2023-07-25T18:10:51.438086Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing mobileNet_V2's Preprocessing","metadata":{}},{"cell_type":"code","source":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","metadata":{"id":"pXR1La8W1Rie","execution":{"iopub.status.busy":"2023-07-25T18:10:51.44113Z","iopub.execute_input":"2023-07-25T18:10:51.442127Z","iopub.status.idle":"2023-07-25T18:10:51.446636Z","shell.execute_reply.started":"2023-07-25T18:10:51.442088Z","shell.execute_reply":"2023-07-25T18:10:51.445509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer Learning from MobileNet_V2","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (224, 224)\nIMG_SHAPE = IMG_SIZE + (3,)\nMobileNet = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","metadata":{"id":"EfX-8SmL1Rif","outputId":"0594a292-0fdc-4dfe-e4cd-fcc7fab1342f","execution":{"iopub.status.busy":"2023-07-25T18:10:51.447952Z","iopub.execute_input":"2023-07-25T18:10:51.448276Z","iopub.status.idle":"2023-07-25T18:10:53.123735Z","shell.execute_reply.started":"2023-07-25T18:10:51.448249Z","shell.execute_reply":"2023-07-25T18:10:53.122589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MobileNet.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:10:53.127288Z","iopub.execute_input":"2023-07-25T18:10:53.127742Z","iopub.status.idle":"2023-07-25T18:10:53.421081Z","shell.execute_reply.started":"2023-07-25T18:10:53.127709Z","shell.execute_reply":"2023-07-25T18:10:53.420101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using MobileNet_V2 in my architecture","metadata":{}},{"cell_type":"markdown","source":"I will build two architectures to see the improvement that I do on the first one","metadata":{}},{"cell_type":"code","source":"def transfer_from_MN(image_shape=IMG_SIZE):\n\n    input_shape = image_shape + (3,)\n\n    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n                                                   include_top=False,\n                                                   weights='imagenet')\n\n    # freeze the base model by making it non trainable\n    base_model.trainable = False\n\n    # create the input layer (Same as the imageNetv2 input size)\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # data preprocessing using the same weights the model was trained on\n    x = preprocess_input(inputs)\n\n    # set training to False to avoid keeping track of statistics in the batch norm layer\n    x = base_model(x, training=False)\n\n    # use global avg pooling to summarize the info in each channel\n    x = tfl.GlobalAveragePooling2D()(x)\n    # include dropout with probability of 0.2 to avoid overfitting\n    x = tfl.Dropout(0.2)(x)\n\n    # use a prediction layer with one neuron (as a binary classifier only needs one)\n    outputs = tfl.Dense(units=20, activation='softmax')(x)\n\n    ### END CODE HERE\n\n    model = tf.keras.Model(inputs, outputs)\n\n    return model","metadata":{"id":"nN0VJ-Xc1Rig","execution":{"iopub.status.busy":"2023-07-25T18:10:53.422222Z","iopub.execute_input":"2023-07-25T18:10:53.422526Z","iopub.status.idle":"2023-07-25T18:10:53.431172Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transfer_from_MN_2(image_shape=IMG_SIZE):\n\n    input_shape = image_shape + (3,)\n\n    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n                                                   include_top=False,\n                                                   weights='imagenet')\n\n    # freeze the base model by making it non trainable\n    base_model.trainable = False\n\n    # create the input layer (Same as the imageNetv2 input size)\n    inputs = tf.keras.Input(shape=input_shape)\n\n    # data preprocessing using the same weights the model was trained on\n    x = preprocess_input(inputs)\n\n    # set training to False to avoid keeping track of statistics in the batch norm layer\n    x = base_model(x, training=False)\n\n    # use global avg pooling to summarize the info in each channel\n    x = tfl.GlobalAveragePooling2D()(x)\n    # include dropout with probability of 0.2 to avoid overfitting\n    x = tfl.Dropout(0.2)(x)\n\n    x = tfl.Dense(units=100)(x)\n    x = tfl.LeakyReLU()(x)\n    x = tfl.Dropout(0.2)(x)\n    x = tfl.Dense(units=100)(x)\n    x = tfl.LeakyReLU()(x)\n    x = tfl.Dropout(0.2)(x)\n    x = tfl.Dense(units=100)(x)\n    x = tfl.LeakyReLU()(x)\n    x = tfl.Dropout(0.2)(x)\n    x = tfl.Dense(units=100)(x)\n    x = tfl.LeakyReLU()(x)\n    x = tfl.Dropout(0.2)(x)\n    # use a prediction layer with one neuron (as a binary classifier only needs one)\n    outputs = tfl.Dense(units=20, activation='softmax')(x)\n\n    ### END CODE HERE\n\n    model = tf.keras.Model(inputs, outputs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:10:53.4327Z","iopub.execute_input":"2023-07-25T18:10:53.433143Z","iopub.status.idle":"2023-07-25T18:10:53.45275Z","shell.execute_reply.started":"2023-07-25T18:10:53.433103Z","shell.execute_reply":"2023-07-25T18:10:53.451936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training and evaluating the model","metadata":{}},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    CIFAR_Recognizer_1 = transfer_from_MN(IMG_SIZE)\n    CIFAR_Recognizer_1.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    CIFAR_Recognizer_2 = transfer_from_MN_2(IMG_SIZE)\n    CIFAR_Recognizer_2.compile(optimizer=tf.keras.optimizers.Adam(),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n","metadata":{"id":"__DhDVHs1Rih","execution":{"iopub.status.busy":"2023-07-25T18:10:53.45372Z","iopub.execute_input":"2023-07-25T18:10:53.453986Z","iopub.status.idle":"2023-07-25T18:11:23.274299Z","shell.execute_reply.started":"2023-07-25T18:10:53.453963Z","shell.execute_reply":"2023-07-25T18:11:23.2733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16 * strategy.num_replicas_in_sync)\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(16 * strategy.num_replicas_in_sync)\ninitial_epochs = 50","metadata":{"id":"3OTbq_p91Rih","outputId":"83c93b86-1486-4d26-cfe3-e1be04f5bdc5","execution":{"iopub.status.busy":"2023-07-25T18:11:23.275465Z","iopub.execute_input":"2023-07-25T18:11:23.27574Z","iopub.status.idle":"2023-07-25T18:11:29.677338Z","shell.execute_reply.started":"2023-07-25T18:11:23.275716Z","shell.execute_reply":"2023-07-25T18:11:29.676Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history1 = CIFAR_Recognizer_1.fit(train_dataset, validation_data=test_dataset, batch_size=16 * strategy.num_replicas_in_sync,epochs=initial_epochs,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:11:29.678938Z","iopub.execute_input":"2023-07-25T18:11:29.67929Z","iopub.status.idle":"2023-07-25T18:30:27.16353Z","shell.execute_reply.started":"2023-07-25T18:11:29.679258Z","shell.execute_reply":"2023-07-25T18:30:27.162176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history2 = CIFAR_Recognizer_2.fit(train_dataset, validation_data=test_dataset, batch_size=16 * strategy.num_replicas_in_sync,epochs=initial_epochs,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:30:27.165531Z","iopub.execute_input":"2023-07-25T18:30:27.165886Z","iopub.status.idle":"2023-07-25T18:49:44.882814Z","shell.execute_reply.started":"2023-07-25T18:30:27.165854Z","shell.execute_reply":"2023-07-25T18:49:44.881508Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models' performance¶","metadata":{}},{"cell_type":"code","source":"plt.plot(history1.history[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:49:44.884829Z","iopub.execute_input":"2023-07-25T18:49:44.885197Z","iopub.status.idle":"2023-07-25T18:49:45.234273Z","shell.execute_reply.started":"2023-07-25T18:49:44.885164Z","shell.execute_reply":"2023-07-25T18:49:45.23325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history2.history[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:49:45.235472Z","iopub.execute_input":"2023-07-25T18:49:45.235749Z","iopub.status.idle":"2023-07-25T18:49:45.426241Z","shell.execute_reply.started":"2023-07-25T18:49:45.235725Z","shell.execute_reply":"2023-07-25T18:49:45.425257Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving the model¶","metadata":{}},{"cell_type":"code","source":"tf.keras.models.save_model(CIFAR_Recognizer_2, '/kaggle/working/model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-07-25T18:49:45.427334Z","iopub.execute_input":"2023-07-25T18:49:45.427648Z","iopub.status.idle":"2023-07-25T18:49:46.564471Z","shell.execute_reply.started":"2023-07-25T18:49:45.427619Z","shell.execute_reply":"2023-07-25T18:49:46.563373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downloading the model","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = os.curdir, file_name = 'model.zip'):\n    \"\"\"\n    zip all the files in a directory\n\n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n\n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n\n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{"id":"-ZfiiiOm1Rii","execution":{"iopub.status.busy":"2023-07-25T18:49:46.565781Z","iopub.execute_input":"2023-07-25T18:49:46.566073Z","iopub.status.idle":"2023-07-25T18:49:46.573563Z","shell.execute_reply.started":"2023-07-25T18:49:46.566048Z","shell.execute_reply":"2023-07-25T18:49:46.572668Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_dir()","metadata":{"id":"eZgel6BR1Rii","outputId":"fb5670e2-8220-4a64-d81a-7fdad37ee91f","execution":{"iopub.status.busy":"2023-07-25T18:49:46.574532Z","iopub.execute_input":"2023-07-25T18:49:46.57479Z","iopub.status.idle":"2023-07-25T18:49:46.63069Z","shell.execute_reply.started":"2023-07-25T18:49:46.574767Z","shell.execute_reply":"2023-07-25T18:49:46.629733Z"},"trusted":true},"outputs":[],"execution_count":null}]}